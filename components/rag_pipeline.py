from langchain_ollama import ChatOllama
from langgraph.graph import StateGraph
from typing import TypedDict
from components.retriever import get_vectorstore
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.tools import Tool
from mcp import ClientSession
from mcp.client.streamable_http import streamablehttp_client
import asyncio
import json
import re

llm = ChatOllama(
    model="mistral:7b",
    base_url="http://localhost:11434"
)

vector_store = get_vectorstore()
retriever = vector_store.as_retriever(search_type="similarity", k=5)

prompt = ChatPromptTemplate.from_messages([
    ("system", "You are an intelligent multilingual assistant. Respond in the same language as the user's question."),
    MessagesPlaceholder(variable_name="chat_history"),
    ("human", "User Question:\n{question}\n\nContext:\n{context}\n\nBased on the context, answer accurately and clearly in the user's language. If the context is insufficient, say so.")
])

class AgentState(TypedDict):
    input: str
    rag_output:list
    context: str
    chat_history: list
    planned_input: str
    final_output: str
    web_output:str

# MCP TOOL CALL FUNCTIONS
async def rag_tool_func(state: AgentState) -> AgentState:
    question = state["input"]
    chat_history = state.get("chat_history", [])
    async with streamablehttp_client("http://localhost:8000/mcp") as (read, write, _):
        async with ClientSession(read, write) as session:
            await session.initialize()
            result = await session.call_tool("rag_search", arguments={"query": question,'chat_history':chat_history})

            state['rag_output']=result.content
            return state

async def web_tool_func(state: AgentState) -> AgentState:

    question = state["input"]

    async with streamablehttp_client("http://localhost:8000/mcp") as (read, write, _):
        async with ClientSession(read, write) as session:
            await session.initialize()
            result = await session.call_tool("web_search", arguments={"query": question})
            state["web_output"] = result.content

            return state

rag_tool = Tool.from_function(
    func=lambda q: asyncio.run(rag_tool_func(q)),
    name="rag_search",
    description="Useful for retrieving context from internal documents."
)

web_tool = Tool.from_function(
    func=lambda s: asyncio.run(web_tool_func(s)),
    name="web_search",
    description=(
        "Use this tool to find current or recent factual information from the internet. Ideal for news, sports scores, weather, or recent events."
        "Ideal for answering questions related to current events, news updates, trending topics, weather, or any facts that may have changed over time. "
        "Do not use this for historical, theoretical, or static information."
    )
)

llm_with_tools = llm.bind_tools([rag_tool, web_tool])

def planner_node(state: AgentState) -> AgentState:
    input_text = state.get("input", "").strip()
    planned_query = input_text.lower()
    state["planned_input"] = planned_query
    return state

def output_node(state: AgentState) -> AgentState:
    question = state.get("input", "")
    agent_output = state.get("web_output") or state.get("rag_output")
    chat_history = state.get("chat_history", [])

    final_prompt = ChatPromptTemplate.from_template("""
Given the user's question, the previous output generated by an agent (RAG, base model, or web search), 
and the conversation history (if any), generate a final response that is clear, polite, and helpful.

Your response should be in the **same language as the user's question**, unless otherwise specified.

Be sure to directly address the user's question based on the provided information. If the agent's output is already good, you may rephrase or polish it. Otherwise, restructure it or explain better based on context.

User Question:
{question}

Agent Output:
{agent_output}

Conversation History (optional):
{chat_history}

Now write the final response for the user:
""")

    full_prompt = final_prompt.invoke({
        "question": question,
        "agent_output": agent_output,
        "chat_history": chat_history
    })
    final_response = llm.invoke(full_prompt)
    state["final_output"] = final_response.content
    return state

def format_web_results(results: list[dict]) -> str:
    formatted = []
    for r in results:
        title = r.get("title", "")
        content = r.get("content", "")
        url = r.get("url", "")
        formatted.append(f"Title: {title}\nContent: {content}\nSource: {url}")

    return "\n\n---\n\n".join(formatted)

def llm_tool_node(state: AgentState) -> AgentState:
    question = state["planned_input"]
    chat_history = state.get("chat_history", [])

    tool_prompt = ChatPromptTemplate.from_messages([
        ("system", "You are a helpful assistant. Use tools if necessary."),
        MessagesPlaceholder(variable_name="chat_history"),
        ("human", "{question}")
    ])

    prompt_value = tool_prompt.invoke({
        "question": question,
        "chat_history": chat_history
    })

    llm_response = llm_with_tools.invoke(prompt_value)
    match = re.search(r'\[.*\]', llm_response.content, re.DOTALL)

    json_part = match.group(0)
    parsed = json.loads(json_part)
    parsed=parsed[0]
    if isinstance(parsed, dict) and parsed.get("name"):
        tool_call = parsed
        tool_name = tool_call["name"]


        if tool_name == "web_search":
            tool_result = web_tool.func(state)
            tool_result = json.loads(tool_result['web_output'][0].text)
            agent_output_text =format_web_results(tool_result["rows"]["results"])
            state["web_output"] = agent_output_text


        elif tool_name == "rag_search":
            tool_result = rag_tool.func(state)
            tool_result=json.loads(tool_result['rag_output'][0].text)
            state["rag_output"] = tool_result['context']

        else:
            state["final_output"] = f"[!] Unknown tool: {tool_name}"
            return state

    return state

graph = StateGraph(AgentState)
graph.set_entry_point("planner")
graph.add_node("planner", planner_node)
graph.add_node("llm_tool_node", llm_tool_node)
graph.add_node("output", output_node)
graph.add_edge("planner", "llm_tool_node")
graph.add_edge("llm_tool_node", "output")
graph.set_finish_point("output")
search_agent = graph.compile()